{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Table of Contents  \n",
    "1  Resumen teórico  \n",
    "1.1  Definición formal  \n",
    "1.2  Funciones kernel  \n",
    "1.3  Outliers  \n",
    "1.4  Preprocesado de datos  \n",
    "1.4.1  Atributos categóricos  \n",
    "1.4.2  Escalado  \n",
    "1.5  Selección del modelo  \n",
    "1.6  Cross-validation  \n",
    "2  Notas a la implementación  \n",
    "3  Análisis del fichero s  \n",
    "3.1  Cargar los ficheros s-train/test  \n",
    "3.2  Seleccionar las 3 clases más frecuentes  \n",
    "3.3  Obtención de un clasificador MVS lineal en el espacio de parámetros  \n",
    "3.4  Realizar una búsqueda similar para núcleos polinómicos de grado 2  \n",
    "3.5  Obtención de un clasificador MVS lineal en el espacio proyectado mediante un núcleo gaussiano  \n",
    "4  Probar distintos tipos de nucleos con un problema no separable linealmente  \n",
    "4.1  Gausianas con escaso solape - separables linealmente  \n",
    "4.2  Gausianas fuertemente solapadas - no separables  \n",
    "4.3  Gausianas separables polinómicamente  \n",
    "5  Utilizar el resto de conjuntos y discutir los resultados  \n",
    "5.1  Conjunto c  \n",
    "5.2  Conjunto p  \n",
    "5.3  Conjunto v  \n",
    "5.4  Conjunto h  \n",
    "5.5  Conjunto z  \n",
    "5.6  Análisis de resultados en los conjuntos c, p, v, h y z  \n",
    "6  Referencias  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rubricas**  \n",
    "Breve resumen teórico 0,313  \n",
    "Objetivo 0,313  \n",
    "Figuras de la técnica (arquitectura) 0,313  \n",
    "Referencias 0,313  \n",
    "Matriz de confusión 0,625  \n",
    "Centroides 0,313  \n",
    "Validación 0,313  \n",
    "Estudio 2,500  \n",
    "Estudio 2,500  \n",
    "Estudio 1,250  \n",
    "Estudio 0,625  \n",
    "Estudio 0,625  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "1. Cargar el archivo iris.srff, hacer clustering con SimpleKMeans con 3 grupos (== 3 clases) y el parámetro seeds por defecto. \n",
    "  1. Utilizar todos los datos para el entrenamiento. Representar gráficamente los resultados.\n",
    "  2. Utilizar 2/3 para entrenamiento y 1/3 para validación. Representar gráficamente los resultados y compararlos con el anterior.\n",
    "  3. Mostrar en una tabla y comparar las coordenadas de los centroides obtenidos utilizando todos los ejemplos de la base de datos frente a los obtenidos s´olo utilizando los ejemplos de entrenamiento.\n",
    "  4. representar, por ejemplo, cuatro gráficas utilizando distintos pares de atributos (eje Y-eje X) y la variable cluster como color. Finalmente, representar tambi´en la variable cluster (eje Y) frente a la variable Instance number (eje X). Comentar los resultados.\n",
    "\n",
    "2. Interpretación de resultados gráficos conocido el valor de clase\n",
    "  1. Enfrentar en una gráfica las intancias, los clusters y el acierto o error en la clasificación.\n",
    "  2. Comentar la bondad de las agrupaciones obtenidas.\n",
    "  3. Mostrar la matriz de confusión asociada al análisis realizado.\n",
    "  \n",
    "3. El método de las k-Medias es sensible a la inicialización aleatoria de centroides (prototipos). \n",
    "  1. Para comprobar este hecho, ejecutar el análisis que se hizo en el apartado anterior modificando cuatro veces (por ejemplo: 51, 88, 99, 139) el valor de la semilla.\n",
    "  2. Mostrar las mismas gráficas que en el apartado 2, comparar los resultados y comentarlos\n",
    "\n",
    "4. Realizar, por ejemplo, cuatro análisis con distintos descartes de atributos.\n",
    "  1.  Mostrar las mismas gráficas que en el apartado 2, comparar los resultados y comentarlos\n",
    "  2. ¿Qué ocurre si sólo se seleccionan para el análisis los atributos “petallength” y “petalwidth”?.\n",
    "  \n",
    "5. Repetir el tipo de análisis realizado en el apartado 2 para tres valores (por ejemplo: 2, 3 y 4) distintos del parámetro k == numero de clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Resumen teórico  \n",
    "  1.  Explicación intuitiva\n",
    "  2.  Definición formal\n",
    "  3.  Parametrización\n",
    "  4.  Matriz de confusión\n",
    "2. Ejecución práctica\n",
    "  1. Estudio de iris.arff ignorando la clase\n",
    "    1. Entrenamiento con todos los datos\n",
    "       1. Representar los resultados\n",
    "    2. Entrenamiento con 2/3 de los datos\n",
    "       1. Representar los resultados\n",
    "    3. Comparación de entrenamiento total vs. 2/3\n",
    "       1. Comparación de gráficas\n",
    "       2. Comparación de centroides\n",
    "    4. Representación de cluters por pares de atributos enfrentados\n",
    "    5. Representación de cluters frente a número de instancia\n",
    "  2. Estudio de iris.arff utilizando la clase\n",
    "    1. Representación de clusters, instancias y aciertos\n",
    "    2. Comentario de las agrupaciones obtenidas\n",
    "    3. Matrices de confusión obtenidas\n",
    "  3. Experimentos basados en cambiar la semilla\n",
    "    1. Prueba con 4 semillas distintas\n",
    "    2. Resultados y gráficas obtenidas\n",
    "  4. Descartes de atributos\n",
    "    1. Cuatro experimentos\n",
    "    2. Experimento con “petallength” y “petalwidth”\n",
    "  5. Experimentos basados en cambiar el número de grupos k\n",
    "3. Conclusiones\n",
    "4. Referencias\n",
    "\n",
    "\n",
    "http://bigdata-madesimple.com/possibly-the-simplest-way-to-explain-k-means-algorithm/\n",
    "\n",
    "http://mynu.herokuapp.com/p/https://pdfs.semanticscholar.org/99d0/ea088fb5f545c7ab4a0d77b2df7c68f031ae.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Resumen teórico\n",
    "\n",
    "El algoritmo de **k-medias** (en inglés k-means) es el algoritmo de clustering por vecindad más popular actualmente [3]. \n",
    "\n",
    "K-medias produce particiones de los\n",
    "datos, es decir, una vez aplicado los datos no pueden pertenecer a 2 categorías\n",
    "como por ejemplo en los algoritmos que generan particiones jerárquicas.\n",
    "\n",
    "##Explicación intuitiva\n",
    "\n",
    "K-medias parte de un estado inicial con una serie de patrones sin etiquetar y\n",
    "unos vectores cuyos valores han sido inicializados aleatoriamente que representarán cada una de las k categorías en las que queremos clasificar los patrones[2]. Cada vez que mostramos un nuevo patrón al algoritmo, es comparado con todos los vectores existentes (denominados **centroides**) y se etiquetará con la clase del centroide que esté a menor distancia del patrón.\n",
    "\n",
    "Por tanto ya hemos definido los parametros necesarios para ejecutar el algorimo\n",
    "k-medias:\n",
    "\n",
    "- Un conjunto de patrones de entrenamiento\n",
    "- Un número de grupos k en el que queremos particionar los datos\n",
    "- Una métrica para medir la distancia entre los patrones (normalmente se utiliza la distancia euclidea)\n",
    "\n",
    "![k-means](k_means.png)\n",
    "\n",
    "*Figura 1: Ejemplo gráfico de entrenamiento del algoritmo K-medias para 2 clases*\n",
    "\n",
    "El proceso de entrenamiento se reduce a conseguir que los centroides sean\n",
    "puntos significativos para cada cluster k, esto se consigue de la siguiente forma:\n",
    "\n",
    "1. Para cada patrón del conjutno de entrenamiento, determinar el centroide más cercano y añadir el patrón a la lista de patrones del centroide\n",
    "2. Una vez asignados todos los patrones a su centroide más cercano, volver a calcular los valores de los centroides a partir de los patrones que pertenecen a su lista\n",
    "3. Repetir este proceso hasta que los centroides convergan (se estabilicen y no se muevan)\n",
    "\n",
    "Mediante este proceso dividimos los patrones de entrenamiento en k regiones,\n",
    "cada una con su prototipo en el centro. Aunque el proceso de entrenamiento\n",
    "puede ser largo, **una vez entrenado el sitema clasificar patrones es inmediato**, ya que sólo hay que calcular la distancia entre el patrón y los centroides y asignar el patrón al cluster del centroide más cercano.\n",
    "\n",
    "En la siguiente figura podemos ver la curva que sigue el movimiento de los tres centroides de un ejemplo de entrenamiento de k-means con 3 regiones:\n",
    "\n",
    "![k-means](http://bigdata-madesimple.com/wp-content/uploads/2015/05/image-4.png)\n",
    "\n",
    "*Figura 2: Ejemplo de la evolución de los centroides*\n",
    "\n",
    "Dado que **en la mayoría de casos en los que se utilizaría este algoritmo desconocemos la clase de los patrones**, es difícil establecer métricas para medir la bondad del modelo obtenido. Más tarde veremos el concepto de matriz de confusión.\n",
    "\n",
    "##Parametrización\n",
    "\n",
    "El primer parámetro significativo en el algoritmo k-means es el número de regiones que queremos que se generen. \n",
    "\n",
    "Si estamos en un problema supervisado, en dónde conocemos de antemano el número de clases, lo más normal es generar tantas regiones como clases haya. Si no se conoce de antemano el número de clases posibles, entonces habrá que probar con distintos valores de K para determinar qué numero de regiones dividen el problema de forma apropiada para cada caso de uso.\n",
    "\n",
    "En relación a la posición inicial de los centroides se suelen seguir distintos métodos:\n",
    "\n",
    "- se pueden generar de forma aleatoria\n",
    "- o se pueden elegir k patrones del conjunto de datos y utilizar estos mismos patrones con centroides iniciales\n",
    "- o se pueden asignar aleatorimente los patrones a k grupos y calcular los centroides de cada grupo (ya en el paso 2 del algoritmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Definición formal\n",
    "\n",
    "El algoritmo k-means tiene por objeto resolver la tarea de clustering (agrupamiento) que consiste en, dado un conjunto de datos, generar una serie de grupos de manera que los individuos perteneciente a cada grupo encontrado\n",
    "presenten características similares entre sí y, a su vez, distintas a la de\n",
    "los individuos pertenecientes al resto de grupos.\n",
    "\n",
    "Formalmente, dado un conjunto de observaciones  $(x_1, x_2, ..., x_n)$, donde cada observación es un vertor real de dimensión $d$, el agrupamiento con el algoritmo k-medias intenta particionar las $n$ observacionesen $k (\\le n)$ en $k$ conjuntos $S = \\{S_1, S_2, ..., S_k\\}$ de modo que se minimice la suma de distancias al cuadrado dentro de cada grupo (cluster), es decir, la varianza.\n",
    "\n",
    "$$\\underset{S}{\\mathrm{argmin}} \\sum_{i=1}^{k} \\sum_{x \\in S_i} \\|x -\\mu_i\\|^2$$\n",
    "\n",
    "siendo $\\mu_i$ la media de los puntos del conjunto $S_i$ (el centroide).\n",
    "\n",
    "##Matriz de confusión\n",
    "\n",
    "Si pensamos en k-medias como un clasificador, en el sentido de que patrones\n",
    "similares son etiquetados en la misma clase, un método sencillo para entender\n",
    "como se comporta el clasificador es utilizar una **matriz de confusión**[1].\n",
    "\n",
    "En una matriz de confusión la clase dada por el modelo está escrita en la\n",
    "parte superior mientras que la etiqueta real esta escrita en la parte derecha. Cada celda de la matriz cuenta cuantas instancias de la clase real han sido clasificadas en cada clase por lo que con esta matriz podemos ver de forma sencilla como de a menudo una clase es incorrectamente clasificada. \n",
    "\n",
    "Una clasificación perfecta produciría una matriz con todo ceros excepto la diagonal (suponiendo que los clusters están ordenados en la matriz apropiadamente), mientras que una mala clasificación produciría valores elevados fuera de la diagonal.\n",
    "\n",
    "![confusion-matrix](confusion_matrix.PNG)\n",
    "\n",
    "*Figura 2: Ejemplo de matriz de confusión*\n",
    "\n",
    "Nótese que en el caso de k-means los nombres de las columnas tendrán que ser los distintos grupos (clusters) generados por el algoritmo y que potencialmente tendrán que verse sometidos a reordenación apropiada para que la regla de la diagonal tenga sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Referencias\n",
    "\n",
    "[1] Joshua Eckroth. k-means clustering, 2016.\n",
    "\n",
    "[2] Miguel Garre, Juan José Cuadrado, Miguel A Sicilia, Daniel Rodríguez,\n",
    "and Ricardo Rejas. Comparación de diferentes algoritmos de clustering en\n",
    "la estimación de coste en el desarrollo de software. Revista Espa~nola de\n",
    "Innovación, Calidad e Ingeniería del Software, 3(1):6{22, 2007.\n",
    "\n",
    "[3] José Hernández Orallo and Ma José Ramírez Quintana. Introducción a la\n",
    "Minería de Datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
