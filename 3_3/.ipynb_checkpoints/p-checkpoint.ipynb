{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Máquinas de soporte vectorial\n",
    "\n",
    "1. Resumen teórico 0,313\n",
    "2. Objetivo 0,313\n",
    "3. Figuras de la técnica (separadores) 0,313\n",
    "4. Referencias 0,313\n",
    "5. Sobreajuste 0,625\n",
    "6. Kernel Trick 0,313\n",
    "7. Cross-Validation 0,313\n",
    "8. Estudio 1,875\n",
    "9. Estudio 1,875\n",
    "10. Estudio 3,750\n",
    "11. Estudio 1,250\n",
    "12. Estudio 1,250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen teórico\n",
    "\n",
    "Las máquinas de soporte vectorial son un tipo de algoritmos utilizados principalmente para clasificación y para regresión desarollados por Vladimir Vapnik y su equipo [1].\n",
    "\n",
    "Las máquinas de soporte vectorial funcionan generando un hiperplano que separa el espacio de muestras de forma que, idealmente, patrones de diferentes clases nunca aparezcan en el mismo lado del plano. Como existen infinitos hiperplanos que separen el espacio, se busca aquel cuya distancia entre el hiperplano y los patrones de cada clase sea la máxima posible (ver Figura 1). A los puntos que conforman las dos líneas paralelas al hiperplano, siendo la distancia entre ellas (margen) la mayor posible, se les denomina vectores de soporte. En la fase de entrenamiento de las máquinas de soporte vectorial, nos centraremos en hallar los vectores de soporte empleando técnicas de programación cuadrática [3].\n",
    "\n",
    "Una vez obtenemos el hiperplano podemos utilizar la Máquina en problemas de clasificación, escogiendo patrones que no hayan sido vistos durante la fase de entrenamiento, y aplicando una etiqueta (clase) en función del lado del hiperplano en el que se proyecten.\n",
    "\n",
    "![\"Representación básica de SVM\"](esquema-basico-svm.PNG)\n",
    "*Figura 1: Ejemplo de 2 dimensiones para una máquina de soporte vectorial, obsérvese que la recta roja es la que maximiza la distancia entre los vectores de soporte (los puntos en amarillo)*\n",
    "\n",
    "\n",
    "\n",
    "Los universos a estudiar no se suelen presentar en casos idílicos de dos dimensiones como en el ejemplo anterior, sino que un algoritmo SVM debe tratar con:\n",
    "\n",
    "- Más de dos variables predictoras\n",
    "- Curvas no lineales de separación\n",
    "- Casos donde los conjuntos de datos no pueden ser completamente separados\n",
    "- Clasificaciones en más de dos categorías\n",
    "\n",
    "![\"Patrones no separables linealmente\"](no-separable-linealmente.PNG)\n",
    "*Figura 2: Espacio de 2 dimensiones en el que los patrones no son separables linealmente*\n",
    "\n",
    "### Definición formal\n",
    "\n",
    "> Dado un conjunto de entrenamiento formado por pares de instancias y etiquetas\n",
    "$$(x_i,y_i),  i=1, . . . , l \\text{ donde } x_i \\in R^n, y \\in \\{1, −1\\}^l$$\n",
    ", las SVMs pueden entenderse como la solución al siguiente problema de optimización::\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{w,b,\\epsilon}{\\operatorname{argmin}} & & \\frac{1}{2}w^Tw + C\n",
    "\\sum_1^l{\\epsilon_i} \\\\\n",
    "\\text{sujeto a} & & y_i(w^T \\phi(x_i) + b) \\ge 1- \\epsilon_i \\\\\n",
    " & & \\epsilon_i \\ge 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "Además, $K(x_i, x_j ) ≡ \\phi(x_i)^T \\phi(x_j)$ es llamada la función kernel.\n",
    "\n",
    "*Figura 3. Definición formal de las SVMs según [4]*\n",
    "\n",
    "Los vectores de entrenamiento $x_i$ son asignados a un espacio de mayor dimensiones (incluso infinitas) por la función $\\phi$. La máquina de soporte vectorial encuentra el hiperplano con máximo margen de separación en el espacio dimensional superior. C > 0 es el parámetro de penalización del término de error.\n",
    "\n",
    "### Funciones kernel\n",
    "\n",
    "Cuando el espacio donde trabajamos no permite separar linealmente los patrones de forma perfecta, como se puede observar en la figura 2, es imposible trazar un hiperplano que separe perfectamente los patrones. \n",
    "\n",
    "Por ello entran en juego las llamadas **funciones kernel**, funciones matemáticas que nos permiten proyectar los patrones en un espacio de mayor dimensión que el original dónde estos si son linealmente separables mediante un hiperplano (Figura 3). \n",
    "\n",
    "![\"Kernel de 2 a 3 dimensiones\"](ejemplo-kernel.PNG)\n",
    "*Figura 4: Kernel de 2 a 3 dimensiones dónde los patrones pueden ser perfectamente separados mediante un hiperplano*\n",
    "\n",
    "De nuevo, y como en la gran mayoría de algoritmos de aprendizaje máquina, **si elegimos un modelo (en este caso una función kernel) que se ajuste demasiado bien a los patrones de entrenamiento**, el clasificador perderá capacidad de generalización por lo que tendrá un mal comportamiento ante nuevos patrones y se hablará de **sobreentrenamiento**.\n",
    "\n",
    "Las funciones kernel más comunmente utilizadas son:\n",
    "\n",
    "- lineales: $K(x_i,x_j) = x_i^T x_j$\n",
    "- polinómicas: $K(x_i,x_j) = (\\gamma x_i^T x_j +r)^d, \\gamma \\ge 0$\n",
    "- funciones de base radial (RBF): $K(x_i,x_j) = exp(-\\gamma \\lVert x_i - x_j \\lVert ^2), \\gamma \\ge 0$\n",
    "- sigmoide: $K(x_i,x_j) = \\tanh(\\gamma x_i^T x_j +r) $\n",
    "\n",
    "\n",
    "### Outliers\n",
    "\n",
    "No siempre es posible encontrar una transformación de los datos que permita separarlos linealmente [2] o incluso a veces, no es conveniente.\n",
    "\n",
    "En este caso nos encontramos con instancias en el conjunto de entrenamiento que se situan en lugares del espacio muy separados o aislados de la zona dónde se sitúan la mayoría de instancias de entrenamiento con las que comparten la clase.\n",
    "\n",
    "La presencia de ruido o de **outliers** pueden llegar tambien a provocar soluciones sobreajustadas a los patrones de entrenamiento que no generalizan bien. Para tratar con estas situaciones se crearon los llamados **SVM de margen blando** cuya idea principal es introducir una holgura al margen existente entre los vectores de soporte y el hiperplano que permite que las restricciones no se cumplan de manera estricta. \n",
    "\n",
    "Se introduce por tanto en el proceso de entrenamiento una constante **C** que controlará el tamaño de la holgura.\n",
    "\n",
    "**El parámetro C le indica a la SVM, en el proceso de entrenamiento, el grado de ejemplos mal clasificados permitido.** \n",
    "\n",
    "Para valores grandes de C, la optimización elegirá un hiperplano de margen más pequeño si ese hiperplano hace un mejor trabajo para conseguir que todos los puntos de entrenamiento estén clasificados correctamente. Es decir, se evitan en mayor medida ejemplos mal clasificados aunque ello conduzca a hiperplanos de margen menor.\n",
    "\n",
    "Por el contrario, un valor muy pequeño de C hará que el optimizador busque un hiperplano de separación de mayor margen, incluso si ese hiperplano clasifica erróneamente más puntos. Para valores muy pequeños de C frecuentemente se obtienen ejemplos mal clasificados,  incluso si los datos de entrenamiento son linealmente separables. Sin embargo, el sobreajuste del clasificador es menor y generalmente se obtienen clasificadores que generalizan mejor.\n",
    "\n",
    "### Preprocesado de datos\n",
    "\n",
    "Se suele realizar a dos niveles [4]:\n",
    "\n",
    "#### Atributos categóricos\n",
    "\n",
    "Las SVMs requieren que cada instancia de datos se represente como un vector de números reales. Por lo tanto, si hay atributos categóricos, primero tenemos que convertirlos en datos numéricos. Se recomienda utilizar m números para representar un atributo con m categorías. Solamente uno de los m números es uno, y los otros son cero. Por ejemplo, una categoría de tres atributos como {rojo, verde, azul} puede representarse como (0,0,1), (0,1,0) y (1,0,0).\n",
    "\n",
    "La experiencia indica que si el número de valores en un atributo no es demasiado grande, esta codificación puede ser más estable que usar un solo número.\n",
    "\n",
    "#### Escalado\n",
    "\n",
    "Escalar los datos antes de aplicar las SVMs es muy importante. La mayoría de las consideraciones hechas para redes neuronales también se aplican a las SVMs. \n",
    "\n",
    "La principal ventaja de escalar es evitar que atributos cuyos valores se mueven en rangos numéricos altos dominen a aquellos que se mueven en rangos numéricos más pequeños. Otra ventaja es evitar dificultades numéricas durante el cálculo, dado que los valores del kernel normalmente dependen de los productos internos de vectores de características, p. \n",
    "\n",
    "Se recomienda un escalado lineal de cada atributo al rango [-1, +1] o [0, 1]. Por supuesto, hay usar el mismo método para escalar el los datos de entrenamiento y los de prueba.\n",
    "\n",
    "### Selección del modelo\n",
    "\n",
    "Aunque sólo son cuatro las funciones kernel más comunes, se debe decidir una para entrenar la SVM. Una vez elegido el núcleo, se ajustan los parámetros de penalización C y los parámetros propios del kernel elegido.\n",
    "\n",
    "**Según [4], en general, el núcleo RBF es una primera opción razonable.** \n",
    "\n",
    "Este kernel no correlaciona las muestras en un espacio dimensional superior, a diferencia del núcleo lineal, con lo que se puede manejar el caso cuando la relación entre las etiquetas y los atributos no es lineal. Además, el núcleo lineal es un caso especial de RBF.\n",
    "\n",
    "La segunda razón es el número de hiperparámetros que influyen en la complejidad de la selección del modelo. El núcleo polinomial tiene más hiperparametros que el núcleo RBF.\n",
    "\n",
    "Por último, el núcleo RBF tiene menos dificultades numéricas ya que el resultado  de aplicar la función kernel es siempre menor que 1.\n",
    "\n",
    "Hay algunas situaciones en las que el núcleo RBF no es adecuado. En particular, cuando el número de características es muy grande, se puede utilizar el núcleo lineal.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "Hay dos parámetros para un núcleo RBF: ** C y $\\gamma$ **. No se sabe de antemano que C y $\\gamma$ son los mejores para un problema dado. En consecuencia, algún tipo de estrategia de selección de parámetros debe realizarse. El objetivo es identificar los parámetros C y $\\gamma$ que generen el modelo que mejor clasifique nuestros datos. El proceso de encontrar estos parámetros consiste la fase de entrenamiento de la SVM.\n",
    "\n",
    "Con la técnica de validación cruzada, primero dividimos el conjunto de entrenamiento en v subconjuntos de igual tamaño. Secuencialmente, el modelo se entrena en $v-1$ subconjuntos y se valida el el restante subconjunto. El método de validación cruzada ayuda e prevenir el problema del sobreajuste, pues todas las instancias acaban siendo usadas para entrenamiento y para validación, alternativamente.\n",
    "\n",
    "## Notas a la implementación\n",
    "\n",
    "En el enunciado de la práctica se hace mención explícita a WEKA y a Torch.\n",
    "\n",
    "Sin embargo, entiendo que el lenguaje o la herramienta con la que resuelvan las prácticas es libre y las referencias a WEKA es simplemente por tener una herramienta por defecto para quien no tenga otras preferencias.\n",
    "\n",
    "En este sentido, y espero no estar cometiendo un error, me tomo la libertad de desarrollar esta práctica con python utilizando scikit-learn como librería de machine-learning.\n",
    "\n",
    "## Actividad 1\n",
    "\n",
    "### Cargar los ficheros s-train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "\n",
    "s_train,s_train_meta = loadarff('svm-data/s-train.arff')\n",
    "s_test,s_test_meta   = loadarff('svm-data/s-test.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar las 3 clases más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 classes: ['46', '20', '110']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "classes = np.concatenate((s_train['class'],s_test['class']))\n",
    "class_counts = dict(zip(*np.unique(classes,return_counts=True)))\n",
    "top_three = sorted(class_counts, key=class_counts.get, reverse=True)[0:3]\n",
    "\n",
    "print 'Top 3 classes: %s' % top_three\n",
    "\n",
    "s_top3_train = filter(lambda x: x[-1] in top_three, s_train)\n",
    "s_top3_test  = filter(lambda x: x[-1] in top_three, s_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de un clasificador MVS lineal en el espacio de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Referencias\n",
    "\n",
    "[1] Corinna Cortes and Vladimir Vapnik. Support-vector networks. *Machine\n",
    "learning, 20(3):273–297, 1995.*\n",
    "\n",
    "[2] José Hernández Orallo and Mª José Ramírez Quintana. *Introducción a la\n",
    "Minería de Datos.*\n",
    "\n",
    "[3] Qiang Wu and Ding-Xuan Zhou. Svm soft margin classifiers: linear programming\n",
    "versus quadratic programming. *Neural computation, 17(5):1160–1187,\n",
    "2005.*\n",
    "\n",
    "[4] http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
