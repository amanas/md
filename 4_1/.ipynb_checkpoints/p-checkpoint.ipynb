{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Nociones-sobre-la-distribución-t-de-Student\" data-toc-modified-id=\"Nociones-sobre-la-distribución-t-de-Student-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Nociones sobre la distribución t de Student</a></div><div class=\"lev1 toc-item\"><a href=\"#Nociones-sobre-test-de-hipótesis-sobre-1-población\" data-toc-modified-id=\"Nociones-sobre-test-de-hipótesis-sobre-1-población-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Nociones sobre test de hipótesis sobre 1 población</a></div><div class=\"lev2 toc-item\"><a href=\"#Concepto-de-p-valor\" data-toc-modified-id=\"Concepto-de-p-valor-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Concepto de p-valor</a></div><div class=\"lev2 toc-item\"><a href=\"#Tipos-de-tests-de-hipótesis\" data-toc-modified-id=\"Tipos-de-tests-de-hipótesis-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Tipos de tests de hipótesis</a></div><div class=\"lev2 toc-item\"><a href=\"#Cómo-se-cálcula-el-p-valor\" data-toc-modified-id=\"Cómo-se-cálcula-el-p-valor-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Cómo se cálcula el p-valor</a></div><div class=\"lev2 toc-item\"><a href=\"#Pasos-para-el-diseño-de-un-test-de-hipótesis\" data-toc-modified-id=\"Pasos-para-el-diseño-de-un-test-de-hipótesis-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Pasos para el diseño de un test de hipótesis</a></div><div class=\"lev1 toc-item\"><a href=\"#Nociones-sobre-test-de-hipótesis-sobre-2-poblaciones\" data-toc-modified-id=\"Nociones-sobre-test-de-hipótesis-sobre-2-poblaciones-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Nociones sobre test de hipótesis sobre 2 poblaciones</a></div><div class=\"lev2 toc-item\"><a href=\"#t-test-pareado\" data-toc-modified-id=\"t-test-pareado-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>t-test pareado</a></div><div class=\"lev1 toc-item\"><a href=\"#Nociones-sobre-comparación-y-replicación-de-clasificadores\" data-toc-modified-id=\"Nociones-sobre-comparación-y-replicación-de-clasificadores-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Nociones sobre comparación y replicación de clasificadores</a></div><div class=\"lev2 toc-item\"><a href=\"#Introducción\" data-toc-modified-id=\"Introducción-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Introducción</a></div><div class=\"lev2 toc-item\"><a href=\"#Métodos-de-muestreo\" data-toc-modified-id=\"Métodos-de-muestreo-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Métodos de muestreo</a></div><div class=\"lev2 toc-item\"><a href=\"#Test-de-hipótesis-más-comunmente-aplicados-en-este-problema\" data-toc-modified-id=\"Test-de-hipótesis-más-comunmente-aplicados-en-este-problema-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Test de hipótesis más comunmente aplicados en este problema</a></div><div class=\"lev1 toc-item\"><a href=\"#Caso-práctico\" data-toc-modified-id=\"Caso-práctico-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Caso práctico</a></div><div class=\"lev2 toc-item\"><a href=\"#Descripción-del-experimento\" data-toc-modified-id=\"Descripción-del-experimento-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Descripción del experimento</a></div><div class=\"lev2 toc-item\"><a href=\"#Diseño-del-test-en-el-contexto-del-problema\" data-toc-modified-id=\"Diseño-del-test-en-el-contexto-del-problema-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Diseño del test en el contexto del problema</a></div><div class=\"lev2 toc-item\"><a href=\"#Realización-del-experimento\" data-toc-modified-id=\"Realización-del-experimento-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Realización del experimento</a></div><div class=\"lev3 toc-item\"><a href=\"#Generación-de-particiones-y-bloques\" data-toc-modified-id=\"Generación-de-particiones-y-bloques-531\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Generación de particiones y bloques</a></div><div class=\"lev3 toc-item\"><a href=\"#Evaluación-de-los-clasificadores-en-cada-bloque-de-cada-partición\" data-toc-modified-id=\"Evaluación-de-los-clasificadores-en-cada-bloque-de-cada-partición-532\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Evaluación de los clasificadores en cada bloque de cada partición</a></div><div class=\"lev3 toc-item\"><a href=\"#Resultados-de-las-evaluaciones-ordenados-por-bloque---con-medias-y-desviación-típica\" data-toc-modified-id=\"Resultados-de-las-evaluaciones-ordenados-por-bloque---con-medias-y-desviación-típica-533\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Resultados de las evaluaciones ordenados por bloque - con medias y desviación típica</a></div><div class=\"lev3 toc-item\"><a href=\"#Tabla-de-diferencias-tras-la-ordenación\" data-toc-modified-id=\"Tabla-de-diferencias-tras-la-ordenación-534\"><span class=\"toc-item-num\">5.3.4&nbsp;&nbsp;</span>Tabla de diferencias tras la ordenación</a></div><div class=\"lev3 toc-item\"><a href=\"#Realización-del-t-test\" data-toc-modified-id=\"Realización-del-t-test-535\"><span class=\"toc-item-num\">5.3.5&nbsp;&nbsp;</span>Realización del t-test</a></div><div class=\"lev3 toc-item\"><a href=\"#Conclusiones\" data-toc-modified-id=\"Conclusiones-536\"><span class=\"toc-item-num\">5.3.6&nbsp;&nbsp;</span>Conclusiones</a></div><div class=\"lev1 toc-item\"><a href=\"#Referencias\" data-toc-modified-id=\"Referencias-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Referencias</a></div><div class=\"lev1 toc-item\"><a href=\"#Actividad-4.1:-Ejercicios-de-simulación\" data-toc-modified-id=\"Actividad-4.1:-Ejercicios-de-simulación-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Actividad 4.1: Ejercicios de simulación</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Nociones sobre la distribución t de Student\n",
    "\n",
    "**La distribución t de Student** es una distribución de probabilidad que **se utiliza para estimar los parámetros de una población cuando el tamaño de la muestra es pequeño y/o cuando la varianza de la población es desconocida**.\n",
    "\n",
    "En estadística, **un estadístico (muestral) es una función medible** $T$ que, dada una muestra estadística de valores $(X_1,X_2,...,X_n)$, les asigna un número, $T(X_1,X_2,...,X_n)$, **que sirve para estimar determinado parámetro de la distribución de la que procede la muestra**.\n",
    "\n",
    "Cuando se extrae una muestra de tamaño $n$ de una población que tiene una distribución normal (o casi normal), la media de la muestra puede transformarse en un estadístico $t$, utilizando la ecuación:\n",
    "\n",
    "$$ t = \\frac{ \\overline{x} - \\mu}{s / \\sqrt{n}}  $$\n",
    "\n",
    "dónde \n",
    "\n",
    "- $\\overline{x}$ es la media de la muestra\n",
    "- $\\mu$ es la media de la población\n",
    "- $s$ es la desviación estándar de la muestra\n",
    "- $n$ es el tamaño de la muestra \n",
    "- $n - 1$ son los grados de libertad\n",
    "\n",
    "El estadístico $t$ producido por esta transformación puede asociarse con una función de probabilidad única. \n",
    "\n",
    "**Esta probabilidad acumulativa representa la probabilidad de encontrar una media de muestra menor o igual que $x$, dada una muestra aleatoria de tamaño $n$.**\n",
    "\n",
    "En realidad hay muchas distribuciones t de Student. La forma particular de la distribución t está determinada por sus **grados de libertad**, que se suelen denotar por $v$. Los grados de libertad se refieren al número de observaciones independientes en un conjunto de datos. El número de observaciones independientes es igual al tamaño de la muestra menos uno. Por lo tanto, la distribución t de Student de las muestras de tamaño 8 se describiría por una distribución t con 8 - 1 o 7 grados de libertad. De forma similar, se utilizaría una distribución t con 15 grados de libertad con una muestra de tamaño 16.\n",
    "\n",
    "![Distribución t de Student](student_t_pdf.png)\n",
    "\n",
    "*Figura: distribución de probabilidad t de Student para varios grados de libertad*\n",
    "\n",
    "La distribución de probabilidad  t de Student **tiene las siguientes propiedades**:\n",
    "\n",
    "- La media de la distribución es igual a 0\n",
    "- La varianza es igual a v / (v - 2), donde v es el grado de libertad de la distribución y v> 2.\n",
    "- La varianza es siempre mayor que 1, aunque está cerca de 1 cuando hay muchos grados de libertad. Con infinitos grados de libertad, la distribución t es la misma que la distribución normal estándar.\n",
    "\n",
    "La distribución **t de Student puede usarse si se da alguna de las siguientes condiciones**:\n",
    "\n",
    "- La distribución de la población es normal\n",
    "- La distribución de la población es simétrica, unimodal, sin valores atípicos, y el tamaño de la muestra es de al menos 30\n",
    "- La distribución de la población es moderadamente sesgada, unimodal, sin valores atípicos, y el tamaño de la muestra es de al menos 40\n",
    "- El tamaño de la muestra es mayor que 40, sin valores atípicos\n",
    "\n",
    "La distribución t no se debe utilizar con muestras pequeñas de poblaciones que no son aproximadamente normales.\n",
    "\n",
    "La distribución de probabilidad  t de Student **tiene las siguientes aplicaciones**:\n",
    "\n",
    "- Test de Hipótesis de la media poblacional\n",
    "- Test de Hipótesis de la diferencia entre las medias de dos poblaciones\n",
    "- Test de Hipótesis de la diferencia entre las medias de dos poblaciones con muestras dependientes (paired t-test)\n",
    "- Test de Hipótesis sobre el coeficiente de correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Nociones sobre test de hipótesis sobre 1 población\n",
    "\n",
    "Lost Tests de Hipótesis son una prueba estadística utilizada para determinar si una hipótesis asumida para una muestra de datos es verdadera para toda la población o no. \n",
    "\n",
    "Lost Tests de Hipótesis formulan dos hipótesis opuestas sobre una población: **la hipótesis nula $H_0$ y la hipótesis alternativa $H_1$**. La hipótesis nula es la afirmación de que no hay diferencia entre el estadístico de muestra y el parámetro de la población real y es la que se prueba. Mientras, la hipótesis alternativa es la afirmación que es verdadera si se rechaza la hipótesis nula.\n",
    "\n",
    "**Basándose en los datos de la muestra, el Test de Hipótesis determina si se rechaza o no la hipótesis nula.**\n",
    "\n",
    "Cuando se diseña un test de hipótesis hay que asumir dos tipo de errores, sobre los que podemos definir dos tipos de parámetros:\n",
    "\n",
    "- **Error de tipo I:** la hipótesis nula es rechazada cuando debería ser aceptada. El parámetro $\\alpha$ determina la probabilididad de que se presente este error que estamos dispuestos a asumir en nuestro test\n",
    "- **Error de tipo II:** la hipótesis nula es aceptada cuando debería ser rechazada. El parámetro $\\beta$ determina la probabilididad de que se presente este error que estamos dispuestos a asumir en nuestro test\n",
    "\n",
    "Podemos ver los conceptos anteriores gráficamente:\n",
    "\n",
    "![Tipos de error](Hypothesis_Testing.jpg)\n",
    "\n",
    "*Figura: tipos de error en los test de hipótesis y parámetros asociados*\n",
    "\n",
    "\n",
    "Se definen igualmente:\n",
    "\n",
    "- el **Nivel de Confianza del test**  $(1-\\alpha)$, que representa la probabilidad de aceptar la hipótesis nula cuando es verdadera.\n",
    "- la **Potencia del test**  $(1-\\beta)$,  que representa la probabilidad de rechazar la hipótesis nula cuando es falsa.\n",
    "\n",
    "## Concepto de p-valor\n",
    "\n",
    "Definimos el **p-valor como la probabilidad de que, suponiendo cierta H0, el estadístico de contraste tome un valor al menos tan extremo como el que se obtiene a partir de las observaciones muestrales**, i.e., el p-valor es el área de la cola de la distribución (o colas si el test es bilateral) definida a partir del estadístico de contraste:. Notas:\n",
    "\n",
    "1. El p-valor sólo puede calcularse una vez tomada la muestra, obteniéndose niveles críticos distintos para cada muestra.\n",
    "2. El p-valor puede interpretarse como un nivel mínimo de significación en el sentido de que niveles de significación α , iguales o superiores al p - valor llevarán a rechazar la hipótesis nula. Por tanto, cuanto menor sea el p - valor mayor es el grado de incompatibilidad de la muestra con H0, lo que lleva a rechazar H0.\n",
    "3. El cálculo del p-valor no proporciona de modo sistemático una decisión entre H0 y H1. Esta forma de abordar los tests, nos permite una visión más amplia, por cuanto nos da información de para qué niveles de significación puede rechazarse la hipótesis nula, y para cuales no se puede. \n",
    "\n",
    "\n",
    "**El p-valor nos proporciona el grado de credibilidad de la hipótesis nula:**\n",
    "\n",
    "- si el valor de p fuese “muy pequeño” (inferior a 0,001), significaría que la hipótesis nula es del todo increíble (en base a las observaciones obtenidas), y por tanto la descartaríamos\n",
    "- si el valor de p oscilase entre 0,05 y 0,001 significaría que hay fuertes evidencias en contra de la hipótesis nula, por lo que la rechazaríamos o no en función del valor que hubiésemos asignado (a priori) a α\n",
    "- si el valor de p es “grande” (superior a 0,05), no habría motivos suficientes como para descartar la hipótesis nula, por lo que la tomaríamos como cierta. \n",
    "\n",
    "## Tipos de tests de hipótesis\n",
    "\n",
    "Los hay de una cola o unilaterales (por la izquierda o por la derecha) y de dos colas o bilaterales.\n",
    "\n",
    "- **test de una cola por la derecha:** compara la hipótesis de un valor mayor en el parámetro que el de la hipótesis nula. El nivel de significancia se carga todo hacia el lado derecho, para definir las regiones de aceptación y de rechazo.\n",
    "- **test de una cola por la izquierda:** compara la hipótesis de un valor menor en el parámetro que el de la hipótesis nula. El nivel de significancia se carga todo hacia el lado izquierdo, para definir las regiones de aceptación y de rechazo.\n",
    "- **test bilateral:** compara la hipótesis de un valor igual en el parámetro que el de la hipótesis nula. El nivel de significancia se carga hacia ambos lados, para definir las regiones de aceptación y de rechazo.\n",
    "\n",
    "$$ $$\n",
    "![Test de una cola por la derecha](stat_83.png)\n",
    "$$ $$\n",
    "\n",
    "*Figura: distintos tipos de test de hipótesis*\n",
    "\n",
    "\n",
    "## Cómo se cálcula el p-valor\n",
    "\n",
    "Depende del tipo de contraste (bilateral, unilateral cola izquierda, unilateral cola derecha) y del estadístico del contraste. En cualquier caso, asumimos que vamos a trabajar con la distribución t de Student tal como se indica en el enunciado de la práctica.\n",
    "\n",
    "Para una cola unilateral por la derecha, $H_0: estadístico \\le x; H_1: estadístico \\gt x$, $p = P(t_{n-1} \\gt  t_0)$\n",
    "\n",
    "Para una cola unilateral por la izquierda, $H_0: estadístico \\ge x; H_1: estadístico \\lt x$, $p = P(t_{n-1} \\lt  t_0)$\n",
    "\n",
    "Para una cola bilateral, $H_0: estadístico = x; H_1: estadístico \\ne x$, $p = 2 * P(t_{n-1} \\gt \\vert t_0 \\vert)$\n",
    "\n",
    "## Pasos para el diseño de un test de hipótesis\n",
    "\n",
    "Son los siguientes:\n",
    "\n",
    "1. Identificar el parámetro de interés ($\\mu$ → media poblacional)\n",
    "2. Establecer las hipótesis $H_0$ y $H_1$.\n",
    "3. Fijar un nivel de significación $\\alpha$\n",
    "4. Determinar el estadístico del contraste\n",
    "5. Establecer las regiones de aceptación y rechazo\n",
    "6. Calcular el valor que toma el estadístico del contraste para la muestra seleccionada.\n",
    "7. Decidir si debe o no rechazarse $H_0$ e interpretar la decisión tomada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nociones sobre test de hipótesis sobre 2 poblaciones\n",
    "\n",
    "Un test de hipótesis para dos muestras es similar en muchos aspectos al test para una muestra:\n",
    "\n",
    "- Se especifica una hipótesis nula, en la mayoría de los casos se propone que las medias de las dos poblaciones son iguales y se establece la hipótesis alternativa (uni o bilateral)\n",
    "- Se especifica un nivel de significación $\\alpha$\n",
    "- Se calcula el p-valor: la probabilidad de obtener datos cuyas medias muestrales difieren tanto o más que la diferencia observada cuando $H_0$ es verdadera. Si esta probabilidad es pequeña (menor que $\\alpha$) se rechaza $H_0$ y se concluye que la diferencia observada no es atribuible al azar y las medias de las dos poblaciones son diferentes.\n",
    "\n",
    "**El estadístico del test dependerá de la estructura de los conjuntos de datos. En particular es importante establecer si los datos corresponden a muestras apareadas o independientes.**\n",
    "\n",
    "Dos muestras son independientes o dependientes entre sí, en función de si las observaciones de las muestras se han obtenido de los mismos individuos u objetos o no.\n",
    "\n",
    "Si ambas muestras se obtienen de distintos individuos, máquinas, empresas, objetos, etc., no hay nada en común en dichas muestras lo que hace que ambas sean **“independientes”**.\n",
    "\n",
    "Sin embargo, si las observaciones o valores de ambas muestras se obtienen de los mismos individuos, empresas, agentes, etc. (potencialmente sujetos a algún tipo de acción entre muestra y muestra), diremos que hay algo en común en dichas muestras por lo que serán muestras **“dependientes” o “no independientes”**. \n",
    "\n",
    "\n",
    "## t-test pareado\n",
    "\n",
    "Si se quiere comparar estadísticos sobre dos poblaciones distintas, una forma de hacerlo es hacer un t-test de una población sobre la muestra resultante de practicar las diferencias a las dos muestras de las 2 poblaciones originales. Esto es lo que se conoce como paired t-test.\n",
    "\n",
    "Atendiendo a la enciclopedia matemática on-line [Mathworld](http://mathworld.wolfram.com/Pairedt-Test.html), dados dos conjuntos $X_i$ e $Y_i$ de n muestras cada uno, el **t-test pareado** determina si se diferencian unas de otras de una manera significativa bajo las suposiciones de que las diferencias de los pares son independientes y normalmente distribuidas.\n",
    "\n",
    "Para aplicar la prueba, se define:\n",
    "\n",
    "- $\\hat{X}_{i} = (X_{i} - \\overline{X})$\n",
    "- $\\hat{Y}_{i} = (Y_{i} - \\overline{Y})$\n",
    "\n",
    "de dónde se construye el estadístico:\n",
    "\n",
    "$$t = (\\overline{X} - \\overline{Y}) \\sqrt{\\frac{n (n-1)}{\\sum_{i=1}^n (\\hat{X}_{i} - \\hat{Y}_{i})^2}}$$\n",
    "\n",
    "Este estadístico tiene una distribución t-Student con n-1 grados de libertad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nociones sobre comparación y replicación de clasificadores\n",
    "\n",
    "Resumimos aquí las partes más relevantes del artículo **Bouckaert, R. (2004). Estimating Replicability of Classifier Learning Experiments, ICML,** en relación con los ejercicios pedidos en esta práctica.\n",
    "\n",
    "## Introducción\n",
    "\n",
    "> **Definición**: la **replicabilidad de un experimento** es la probabilidad de dos ejecuciones del experimento en el mismo conjunto de datos, con el mismo par de algoritmos y el mismo método de muestreo de los datos produzca el mismo resultado.\n",
    "\n",
    "El problema que se aborda es, dados dos algoritmos de aprendizaje $A$ y $B$ que generan clasificadores y un pequeño conjunto de datos $D$, cómo tomar una decisión sobre cuál de los dos algoritmos funciona mejor basado en la precisión de clasificación para el conjunto de datos dado. Un método general para tomar tal decisión es dividir $D$ en un conjunto de entrenamiento $D_t$ y un conjunto de prueba $D - D_t$. Luego, se entrena el algoritmo $A$ y $B$ en $D_t$ y se registra la exactitud de clasificación en el $D - D_t$. De esta manera, obtenemos dos precisiones de clasificación $P_A$ y $P_B$ y la diferencia $x = P_A - P_B$ nos da una indicación de qué algoritmo ofrece mejor rendimiento.\n",
    "\n",
    "Una manera formal de tomar tal decisión es aplicar un test de hipótesis. Sin embargo, esto requiere más de un solo resultado $x$. Lo podemos conseguir dividiendo $D$ repetidamente en los conjuntos de entrenamiento y de prueba para obtener resultados múltiples $P_{A,i}$ y $P_{B,i}$ con diferencias asociadas $x_i = P_{A,i} - P_{B,i}, 1 \\le i \\le n$ obteniendo una muestra de tamaño $n$.\n",
    "\n",
    "Por lo tanto, un experimento tiene dos componentes:\n",
    "\n",
    "- en primer lugar, un esquema de muestreo para obtener una muestra $x_1,... ,x_n$\n",
    "- y en segundo lugar, un test de hipótesis para tomar una decisión basada en la muestra\n",
    "\n",
    "\n",
    "## Métodos de muestreo\n",
    "\n",
    "Recordemos que lo que estamos muestreando son rendimientos de dos algoritmos sobre un mismo conjunto de datos.\n",
    "\n",
    "- **Resampling:** consiste en dividir aleatoriamente $n$ veces el conjunto $D$ en $D_t$ (para prueba) y $D-D_t$ (para entrenamiento). \n",
    "\n",
    "- **K-fold cross validation:** divide $D$ en $k$ partes aproximadamente iguales $D_1,...,D_k$. En la ejecución $i$ se entrena en $D_j, j\\ne i$y se evalúa en $D_i$. \n",
    "\n",
    "- **K-fold cross validation repetida:** consiste en repetir el método k-fold anterior varias veces.\n",
    "\n",
    "- **Average over folds:** consiste en practicar agragaciones sobre todos los folds de una ejecución en un muestreo de tipo K-fold cross validation repetido. *Agregaciones en horizontal.*\n",
    "\n",
    "- **Average over runs:** consiste en practicar agragaciones sobre todas las ejecuciones para un mismo fold en un muestreo de tipo K-fold cross validation repetido. *Agregación en vertical.*\n",
    "\n",
    "- **Average over sorted runs:** las agregaciones sobre ejecuciones en  un mismo fold combinan los resultados de diferentes ejecuciones de forma arbitraria. Se obtienen mejores estimaciones de un experimento si primero se ordenan los resultados de los folds de cada ejecución y luego se hacen los promedios por ejecución. Es decir, primero ordenamos en horizontal (ordenamos los resultados de los folds de cada ejecución) y luego promediamos en vertical. \n",
    "\n",
    "**En el artículo se indica que el método de muestreo ordenado es el único que ha ofrecido errores aceptables de Tipo I y potencia razonable para una amplia gama de parámetros utilizando las tres pruebas de hipótesis consideradas, que se explican en el siguiente punto.**\n",
    "\n",
    "\n",
    "\n",
    "## Test de hipótesis más comunmente aplicados en este problema\n",
    "\n",
    "En el experimento tenemos las siguientes hipótesis:\n",
    "\n",
    "- $H_0$: los algoritmos $A$ y $B$ ofrecen el mismo rendimiento\n",
    "- $H_1$: los algoritmos $A$ y $B$ no ofrecen el mismo rendimiento\n",
    "\n",
    "O dicho de otro modo, queremos probar si $x_1,...,x_n$ tienen una media igual a 0. Hay distintos métodos para contrastar esta conjecuta, cada uno de ellos sujeto a distintas suposiciones.\n",
    "\n",
    "- **t-test pareado:** asume que las diferencias $x_i$ se distribuyen normalmenta. El test se fundamente en un estimador con distribución t-student con $n-1$ grados de libertad.\n",
    "- **Sign test:** no asume nada sobre los datos. Se basa en contar el número de valores positivos en la muestra. El test se fundamenta en un etimador con distribución binomial.\n",
    "- **Rank sum test:** no asume nada sobre los datos. Se fundamenta en el tamaño de los valores absolutos de los datos y se basa en un estimador con distribución normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso práctico\n",
    "\n",
    "## Descripción del experimento\n",
    "\n",
    "En este apartado se ponen en práctica las nociones teóricas explicadas en los apartados anteriores sobre comparación de clasificadores  sobre un mismo conjunto de datos.\n",
    "\n",
    "A tal efecto se utiliza el conocido conjunto **iris** que contiene 150 muestras de tres tipos de lirio (Setosa, Versicolor y Virginia) - 50 muestras por cada tipo. En cada muestra se tienen las propiedades longitud y anchura del pétalo y del sépalo.\n",
    "\n",
    "Se van a generar **10 particiones de 10 bloques** cada una del citado conjunto de datos. Para cada partición, se va a realizar un experimento de **validación cruzada con un clasificador basado en redes bayesianas y otro en árboles de decisión,** y se van a **ordenar las diferencias de rendimiento** de mayor a menor en una lista, según el ejemplo *Average over sorted runs del artículo Bouckaert, R. (2004). Estimating Replicability of Classifier Learning Experiments, ICML,* disponible [aquí](http://www.aicml.cs.ualberta.ca/_ban_04/icml/pages/papers/61.pdf).\n",
    "\n",
    "Se realizará un **test t de Student** diseñado en el contexto particular de este problema y se determinarán si existen diferencias estadísticas entre los resultados obtenidos por los dos clasificadores. Se ofrecen **tablas de resultados de los distintos experimentos y críticas a los resultados.**\n",
    "\n",
    "## Diseño del test en el contexto del problema\n",
    "\n",
    "Tenemos dos clasificadores:\n",
    "\n",
    "- $A$, basado en redes bayesianas\n",
    "- $B$, basado en árboles de decisión\n",
    "\n",
    "Tenemos un conjunto de datos, sobre el que generamos:\n",
    "\n",
    "- 10 particiones, referenciadas por $i$. Se corresponden con 10 distintas ejecuciones del experimento\n",
    "- 10 bloques en cada partición, referenciados por $j$. Se correspondel las 10 validaciones cruzadas hechas en cada ejecución del experimento\n",
    "\n",
    "Cada validación cruzada realizada por cada algoritmo en cada ejecución devolverá un resultado del rendimiento del algoritmo en el bloque, que denotaremos por $P_{aij}, a \\in \\{A,B\\}, 1 \\le i \\le 10, 1 \\le j \\le 10$.\n",
    "\n",
    "Mostraremos en una tabla, a título informativo, los valores $P_{aij}$ anteriores.\n",
    "\n",
    "Ordenaremos por filas de menor a mayor los valores de rendimiento $P_{aij}$ anteriores, obteniendo valores que denominaremos $Ps_{aij}$ dónde $s$ significa que ya estamos sujetos a ordenación, y los mostraremos a título informativo también.\n",
    "\n",
    "Posteriormente, calcularemos las diferencias de rendimiento de ambos algoritmos sobre sus rendimientos ordenados por bloques de la partición. De este modo obtendremos los valores $x_{ij} = Ps_{Aij} - Ps_{Bij}$.\n",
    "\n",
    "Y con estos valores $x_i, 1 \\le i \\le 10$ llevaremos a cabo nuestro test de contraste de hipótesis para ver si A y B ofrecen o no el mismo rendimiento con cierto grado de evidencia. Para esto necesitamos, por lo tanto, definir más cosas.\n",
    "\n",
    "**Realizaremos un t-test pareado sobre las muestras $x_i$ obtenidas. ** Este test , tal como se explicó más arriba, nos permite comparar dos poblaciones analiando las diferencias de las muestras.  \n",
    "\n",
    "<font color=\"#ff0000\">\n",
    "**TODO:** para ello, primeramente, hay que demostrar que los valores de las $x_i$ siguen una distribución normal. Realizaremos un test de normalidad para garantizar tal requisito.\n",
    "</font>\n",
    "\n",
    "Por lo demás, pese a que es necesario que los valores $x_i$ sean mutuamente independientes,  obviamente no ocurre pues un mismo $x_i$ podría aparecer varias veces, el artículo indica que podemos proceder obviando este requisito.\n",
    " \n",
    "A tal efecto, **las hipótesis a contrastar** serán:\n",
    "\n",
    "- $H_0$: $A$ y $B$ ofrecen el mismo rendimiento\n",
    "- $H_1$: $A$ y $B$ no ofrecen el mismo rendimiento\n",
    "\n",
    "El estadístico de contraste en este tipo de test es:\n",
    "\n",
    "$$ Z = \\frac{\\hat{m}}{\\hat{\\sigma} / \\sqrt{df+1}} $$\n",
    "\n",
    "siendo:\n",
    "\n",
    "- $\\hat{m} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$\n",
    "- $\\hat{\\sigma} = \\sqrt{ \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\hat{m})^2} $\n",
    "- $df = n-1$, grados de libertad\n",
    "- $n = 10$, número de diferencias ordenadas de rendimiento que tenemos de los puntos anteriores\n",
    "- $x_i$, i-ésima diferencia ordenada de rendimiento de los clasificadores $A$ y $B$\n",
    "\n",
    "que sigue una distribución t-Student $P_t$ con $n-1=9$ grados de libertad.\n",
    "\n",
    "**El test es de dos colas**, porque lo mismo nos dá que los tests no tengan al mismo rendimiento porque $A$ es mejor que $B$ que porque $B$ sea mejor que $A$. O dicho de otro modo, contrastamos que la media sea 0 frente a que no lo sea.\n",
    "\n",
    "Probaremos con **valores de significancia $\\alpha=0.1, \\alpha=0.05$ y $\\alpha=0.01$.**. \n",
    "\n",
    "Para estas significancias, atendiendo a la distribución t-Student de 9 grados de libertad, determinamos los **intervalos de confianza** que tendremos que tener en cuanta en el contraste. Muy importante observar que estamos en el caso de dos colas, que es el funcionamiento por defecto de la función interval de scipy.stats.t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.1  two tailed confidence i.: [-1.833, 1.833]\n",
      "alpha=0.05 two tailed confidence i.: [-2.262, 2.262]\n",
      "alpha=0.01 two tailed confidence i.: [-3.250, 3.250]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "print 'alpha=0.1  two tailed confidence i.: [%.3f, %.3f]' % t.interval(1 - 0.1, 9)\n",
    "print 'alpha=0.05 two tailed confidence i.: [%.3f, %.3f]' % t.interval(1 - 0.05, 9)\n",
    "print 'alpha=0.01 two tailed confidence i.: [%.3f, %.3f]' % t.interval(1 - 0.01, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realización del experimento\n",
    "\n",
    "### Generación de particiones y bloques\n",
    "\n",
    "Afortunadamente, en [sciki-learn](http://scikit-learn.org) ya disponemos del conjunto \"iris\" y de funcionalidad para crear bloques. Por cierto que me parece conveniente crear los bloques con estratificación, es decir, que la presencia de las clases en cada bloque sea uniforme. eso sí, para evitar repetir los mismos bloques en cada partición, aplico el parámetro de aleatoriedad.\n",
    "\n",
    "Básicamente vamos a generar un **array de 10 particiones**. Cada elemento del array de particiones será un **array de 10 bloques**. Cada elemento del array de bloques está compuesto por dos elementos: el primero con los **índices de las instancias que se usan para entrenamiento** y el segundo con los **índices de las instancias que se usan para prueba**.\n",
    "\n",
    "Igualmente, para que esta práctica pueda ser ejecutada varias veces (si se desea) sobre el mismo conjunto de dato, me aseguro de que los datos puedan ser reutilizados con la siguiente función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def create_or_load_particiones():\n",
    "\n",
    "    file_name = 'particiones.pkl'\n",
    "    \n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as input:\n",
    "            return pickle.load(input)\n",
    "        \n",
    "    particiones = []\n",
    "    iris = datasets.load_iris()\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    for i in range(10):\n",
    "        particiones += [list(skf.split(iris.data, iris.target))]\n",
    "    with open(file_name, 'wb') as output:\n",
    "        pickle.dump(particiones, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return particiones\n",
    "\n",
    "particiones = create_or_load_particiones()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de los clasificadores en cada bloque de cada partición\n",
    "\n",
    "Instanciamos un clasificador bayesiano y un arbol de decisión sin optimización alguna - optimizar los clasificadores no es el objetivo de esta práctica sino comparar sus rendimientos.\n",
    "\n",
    "Posteriormente los entrenamos y evaluamos en todas las particiones y en todos los bloques. Guardamos el resultado en un fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "def create_or_load_evaluations():\n",
    "    \n",
    "    file_name = 'evaluations.pkl'\n",
    "    \n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as input:\n",
    "            return pickle.load(input)\n",
    "        \n",
    "    dtc = DecisionTreeClassifier()\n",
    "    gnb = GaussianNB()\n",
    "\n",
    "    dtc_evaluations = numpy.zeros(shape=(10,10))\n",
    "    gnb_evaluations = numpy.zeros(shape=(10,10)) \n",
    "    for i in range(10):\n",
    "        particion = particiones[i]\n",
    "        for j in range(10):\n",
    "            bloque = particion[j]\n",
    "            train_index,test_index = bloque\n",
    "            X_train,X_test = iris.data[train_index],iris.target[train_index]\n",
    "            y_train,y_test = iris.data[test_index],iris.target[test_index]\n",
    "            dtc_evaluations[i,j] = dtc.fit(X_train,X_test).score(y_train,y_test)\n",
    "            gnb_evaluations[i,j] = gnb.fit(X_train,X_test).score(y_train,y_test)    \n",
    "    evaluations = {'dtc': dtc_evaluations, 'gnb': gnb_evaluations}\n",
    "    with open(file_name, 'wb') as output:\n",
    "        pickle.dump(evaluations, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return evaluations\n",
    "\n",
    "evaluations = create_or_load_evaluations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos en forma de tabla las evaluaciones de cada clasificador en cada partición y en cada bloque. Definimos además una función que nos permite exportar a latex un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B0</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P0</th>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P3</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P4</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P5</th>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P6</th>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P7</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P8</th>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9</th>\n",
       "      <td>0.8000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &      B0 &      B1 &      B2 &      B3 &      B4 &      B5 &      B6 &      B7 &      B8 &      B9 \\\\\n",
       "\\midrule\n",
       "P0 &  0.8667 &  1.0000 &  1.0000 &  1.0000 &  0.9333 &  0.9333 &  0.8667 &  0.9333 &  1.0000 &  0.9333 \\\\\n",
       "P1 &  1.0000 &  0.7333 &  0.9333 &  1.0000 &  0.8667 &  1.0000 &  1.0000 &  1.0000 &  0.8667 &  0.9333 \\\\\n",
       "P2 &  0.9333 &  0.9333 &  1.0000 &  0.9333 &  1.0000 &  0.8667 &  1.0000 &  1.0000 &  0.9333 &  0.9333 \\\\\n",
       "P3 &  1.0000 &  1.0000 &  0.9333 &  0.9333 &  1.0000 &  0.9333 &  0.8667 &  1.0000 &  1.0000 &  0.8000 \\\\\n",
       "P4 &  1.0000 &  1.0000 &  0.9333 &  0.9333 &  1.0000 &  0.9333 &  0.8667 &  1.0000 &  0.9333 &  0.9333 \\\\\n",
       "P5 &  0.8667 &  0.9333 &  1.0000 &  0.9333 &  1.0000 &  0.9333 &  1.0000 &  1.0000 &  1.0000 &  0.8667 \\\\\n",
       "P6 &  0.9333 &  0.8667 &  0.9333 &  0.9333 &  1.0000 &  0.9333 &  1.0000 &  0.9333 &  1.0000 &  1.0000 \\\\\n",
       "P7 &  1.0000 &  1.0000 &  1.0000 &  0.9333 &  1.0000 &  0.9333 &  0.8667 &  0.8667 &  0.8667 &  1.0000 \\\\\n",
       "P8 &  0.9333 &  0.9333 &  0.9333 &  1.0000 &  0.8667 &  0.9333 &  1.0000 &  1.0000 &  0.8667 &  1.0000 \\\\\n",
       "P9 &  0.8000 &  1.0000 &  1.0000 &  1.0000 &  1.0000 &  0.8667 &  1.0000 &  1.0000 &  0.9333 &  1.0000 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "        B0      B1      B2      B3      B4      B5      B6      B7      B8  \\\n",
       "P0  0.8667  1.0000  1.0000  1.0000  0.9333  0.9333  0.8667  0.9333  1.0000   \n",
       "P1  1.0000  0.7333  0.9333  1.0000  0.8667  1.0000  1.0000  1.0000  0.8667   \n",
       "P2  0.9333  0.9333  1.0000  0.9333  1.0000  0.8667  1.0000  1.0000  0.9333   \n",
       "P3  1.0000  1.0000  0.9333  0.9333  1.0000  0.9333  0.8667  1.0000  1.0000   \n",
       "P4  1.0000  1.0000  0.9333  0.9333  1.0000  0.9333  0.8667  1.0000  0.9333   \n",
       "P5  0.8667  0.9333  1.0000  0.9333  1.0000  0.9333  1.0000  1.0000  1.0000   \n",
       "P6  0.9333  0.8667  0.9333  0.9333  1.0000  0.9333  1.0000  0.9333  1.0000   \n",
       "P7  1.0000  1.0000  1.0000  0.9333  1.0000  0.9333  0.8667  0.8667  0.8667   \n",
       "P8  0.9333  0.9333  0.9333  1.0000  0.8667  0.9333  1.0000  1.0000  0.8667   \n",
       "P9  0.8000  1.0000  1.0000  1.0000  1.0000  0.8667  1.0000  1.0000  0.9333   \n",
       "\n",
       "        B9  \n",
       "P0  0.9333  \n",
       "P1  0.9333  \n",
       "P2  0.9333  \n",
       "P3  0.8000  \n",
       "P4  0.9333  \n",
       "P5  0.8667  \n",
       "P6  1.0000  \n",
       "P7  1.0000  \n",
       "P8  1.0000  \n",
       "P9  1.0000  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "def _repr_latex_(self):\n",
    "    return self.to_latex()\n",
    "\n",
    "pd.DataFrame._repr_latex_ = _repr_latex_\n",
    "\n",
    "dtc_df = pd.DataFrame(data=evaluations['dtc'],\n",
    "                      columns=['B' + str(i) for i in range(10)],\n",
    "                      index=['P' + str(i) for i in range(10)])\n",
    "dtc_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figura: scoring del clasificador basado en arbol de decisión por partición y bloque*  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B0</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P0</th>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P3</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P4</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.8667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P5</th>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P6</th>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P7</th>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P8</th>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9</th>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &      B0 &      B1 &      B2 &      B3 &      B4 &      B5 &      B6 &      B7 &      B8 &      B9 \\\\\n",
       "\\midrule\n",
       "P0 &  0.8667 &  1.0000 &  1.0000 &  1.0000 &  0.9333 &  1.0000 &  0.9333 &  0.9333 &  1.0000 &  0.9333 \\\\\n",
       "P1 &  1.0000 &  0.9333 &  0.9333 &  0.9333 &  0.8667 &  1.0000 &  1.0000 &  1.0000 &  0.8667 &  0.9333 \\\\\n",
       "P2 &  0.8667 &  1.0000 &  1.0000 &  0.9333 &  1.0000 &  0.8667 &  1.0000 &  1.0000 &  1.0000 &  0.9333 \\\\\n",
       "P3 &  1.0000 &  1.0000 &  0.9333 &  0.9333 &  0.9333 &  0.9333 &  0.8667 &  1.0000 &  1.0000 &  0.9333 \\\\\n",
       "P4 &  1.0000 &  1.0000 &  0.9333 &  1.0000 &  1.0000 &  0.9333 &  0.9333 &  0.9333 &  0.9333 &  0.8667 \\\\\n",
       "P5 &  0.8667 &  0.9333 &  1.0000 &  0.9333 &  1.0000 &  0.9333 &  1.0000 &  1.0000 &  0.9333 &  1.0000 \\\\\n",
       "P6 &  0.9333 &  0.9333 &  1.0000 &  0.9333 &  0.9333 &  0.9333 &  1.0000 &  0.9333 &  1.0000 &  1.0000 \\\\\n",
       "P7 &  0.9333 &  1.0000 &  1.0000 &  0.9333 &  1.0000 &  0.9333 &  1.0000 &  0.8667 &  0.8667 &  1.0000 \\\\\n",
       "P8 &  0.9333 &  0.9333 &  0.9333 &  1.0000 &  0.8667 &  0.9333 &  0.9333 &  1.0000 &  1.0000 &  0.9333 \\\\\n",
       "P9 &  0.8667 &  1.0000 &  1.0000 &  0.9333 &  1.0000 &  0.8667 &  1.0000 &  1.0000 &  0.8667 &  1.0000 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "        B0      B1      B2      B3      B4      B5      B6      B7      B8  \\\n",
       "P0  0.8667  1.0000  1.0000  1.0000  0.9333  1.0000  0.9333  0.9333  1.0000   \n",
       "P1  1.0000  0.9333  0.9333  0.9333  0.8667  1.0000  1.0000  1.0000  0.8667   \n",
       "P2  0.8667  1.0000  1.0000  0.9333  1.0000  0.8667  1.0000  1.0000  1.0000   \n",
       "P3  1.0000  1.0000  0.9333  0.9333  0.9333  0.9333  0.8667  1.0000  1.0000   \n",
       "P4  1.0000  1.0000  0.9333  1.0000  1.0000  0.9333  0.9333  0.9333  0.9333   \n",
       "P5  0.8667  0.9333  1.0000  0.9333  1.0000  0.9333  1.0000  1.0000  0.9333   \n",
       "P6  0.9333  0.9333  1.0000  0.9333  0.9333  0.9333  1.0000  0.9333  1.0000   \n",
       "P7  0.9333  1.0000  1.0000  0.9333  1.0000  0.9333  1.0000  0.8667  0.8667   \n",
       "P8  0.9333  0.9333  0.9333  1.0000  0.8667  0.9333  0.9333  1.0000  1.0000   \n",
       "P9  0.8667  1.0000  1.0000  0.9333  1.0000  0.8667  1.0000  1.0000  0.8667   \n",
       "\n",
       "        B9  \n",
       "P0  0.9333  \n",
       "P1  0.9333  \n",
       "P2  0.9333  \n",
       "P3  0.9333  \n",
       "P4  0.8667  \n",
       "P5  1.0000  \n",
       "P6  1.0000  \n",
       "P7  1.0000  \n",
       "P8  0.9333  \n",
       "P9  1.0000  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_df = pd.DataFrame(data=evaluations['gnb'],\n",
    "                      columns=['B' + str(i) for i in range(10)],\n",
    "                      index=['P' + str(i) for i in range(10)])\n",
    "gnb_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figura: scoring del clasificador basado en redes bayesianas por partición y bloque*  \n",
    "\n",
    "### Resultados de las evaluaciones ordenados por bloque - con medias y desviación típica\n",
    "\n",
    "Implementamos una función que nos haga el trabajo y mostramos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SB0</th>\n",
       "      <th>SB1</th>\n",
       "      <th>SB2</th>\n",
       "      <th>SB3</th>\n",
       "      <th>SB4</th>\n",
       "      <th>SB5</th>\n",
       "      <th>SB6</th>\n",
       "      <th>SB7</th>\n",
       "      <th>SB8</th>\n",
       "      <th>SB9</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P0</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.052587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.088889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.044997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P3</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.068853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P4</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.044997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P5</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.054885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P6</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.044997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P7</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.061262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P8</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.052587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.071665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>0.046614</td>\n",
       "      <td>0.032203</td>\n",
       "      <td>0.028109</td>\n",
       "      <td>0.021082</td>\n",
       "      <td>0.021082</td>\n",
       "      <td>0.035136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &       SB0 &       SB1 &       SB2 &       SB3 &       SB4 &       SB5 &  SB6 &  SB7 &  SB8 &  SB9 &      Mean &       Std \\\\\n",
       "\\midrule\n",
       "P0   &  0.866667 &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.0 &  1.0 &  1.0 &  1.0 &  0.946667 &  0.052587 \\\\\n",
       "P1   &  0.733333 &  0.866667 &  0.866667 &  0.933333 &  0.933333 &  1.000000 &  1.0 &  1.0 &  1.0 &  1.0 &  0.933333 &  0.088889 \\\\\n",
       "P2   &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.0 &  1.0 &  1.0 &  1.0 &  0.953333 &  0.044997 \\\\\n",
       "P3   &  0.800000 &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  1.000000 &  1.0 &  1.0 &  1.0 &  1.0 &  0.946667 &  0.068853 \\\\\n",
       "P4   &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.0 &  1.0 &  1.0 &  1.0 &  0.953333 &  0.044997 \\\\\n",
       "P5   &  0.866667 &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  1.000000 &  1.0 &  1.0 &  1.0 &  1.0 &  0.953333 &  0.054885 \\\\\n",
       "P6   &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.0 &  1.0 &  1.0 &  1.0 &  0.953333 &  0.044997 \\\\\n",
       "P7   &  0.866667 &  0.866667 &  0.866667 &  0.933333 &  0.933333 &  1.000000 &  1.0 &  1.0 &  1.0 &  1.0 &  0.946667 &  0.061262 \\\\\n",
       "P8   &  0.866667 &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.0 &  1.0 &  1.0 &  1.0 &  0.946667 &  0.052587 \\\\\n",
       "P9   &  0.800000 &  0.866667 &  0.933333 &  1.000000 &  1.000000 &  1.000000 &  1.0 &  1.0 &  1.0 &  1.0 &  0.960000 &  0.071665 \\\\\n",
       "Mean &  0.840000 &  0.886667 &  0.920000 &  0.940000 &  0.940000 &  0.966667 &  1.0 &  1.0 &  1.0 &  1.0 &       NaN &       NaN \\\\\n",
       "Std  &  0.046614 &  0.032203 &  0.028109 &  0.021082 &  0.021082 &  0.035136 &  0.0 &  0.0 &  0.0 &  0.0 &       NaN &       NaN \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "           SB0       SB1       SB2       SB3       SB4       SB5  SB6  SB7  \\\n",
       "P0    0.866667  0.866667  0.933333  0.933333  0.933333  0.933333  1.0  1.0   \n",
       "P1    0.733333  0.866667  0.866667  0.933333  0.933333  1.000000  1.0  1.0   \n",
       "P2    0.866667  0.933333  0.933333  0.933333  0.933333  0.933333  1.0  1.0   \n",
       "P3    0.800000  0.866667  0.933333  0.933333  0.933333  1.000000  1.0  1.0   \n",
       "P4    0.866667  0.933333  0.933333  0.933333  0.933333  0.933333  1.0  1.0   \n",
       "P5    0.866667  0.866667  0.933333  0.933333  0.933333  1.000000  1.0  1.0   \n",
       "P6    0.866667  0.933333  0.933333  0.933333  0.933333  0.933333  1.0  1.0   \n",
       "P7    0.866667  0.866667  0.866667  0.933333  0.933333  1.000000  1.0  1.0   \n",
       "P8    0.866667  0.866667  0.933333  0.933333  0.933333  0.933333  1.0  1.0   \n",
       "P9    0.800000  0.866667  0.933333  1.000000  1.000000  1.000000  1.0  1.0   \n",
       "Mean  0.840000  0.886667  0.920000  0.940000  0.940000  0.966667  1.0  1.0   \n",
       "Std   0.046614  0.032203  0.028109  0.021082  0.021082  0.035136  0.0  0.0   \n",
       "\n",
       "      SB8  SB9      Mean       Std  \n",
       "P0    1.0  1.0  0.946667  0.052587  \n",
       "P1    1.0  1.0  0.933333  0.088889  \n",
       "P2    1.0  1.0  0.953333  0.044997  \n",
       "P3    1.0  1.0  0.946667  0.068853  \n",
       "P4    1.0  1.0  0.953333  0.044997  \n",
       "P5    1.0  1.0  0.953333  0.054885  \n",
       "P6    1.0  1.0  0.953333  0.044997  \n",
       "P7    1.0  1.0  0.946667  0.061262  \n",
       "P8    1.0  1.0  0.946667  0.052587  \n",
       "P9    1.0  1.0  0.960000  0.071665  \n",
       "Mean  1.0  1.0       NaN       NaN  \n",
       "Std   0.0  0.0       NaN       NaN  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sorted_df_with_mean_and_std(src_df):          \n",
    "    df = src_df.copy(deep=True)\n",
    "    df.columns = ['SB' + str(i) for i in range(10)]\n",
    "    for row in range(len(src_df)):\n",
    "        df.iloc[row] = sorted(src_df.values[row])\n",
    "    p_means,p_stds = df.mean(axis=1).values,df.std(axis=1).values\n",
    "    df['Mean'] = p_means\n",
    "    df['Std'] = p_stds\n",
    "    b_means,b_stds = df.mean(axis=0).values,df.std(axis=0).values\n",
    "    df.loc['Mean'] = b_means\n",
    "    df.loc['Std'] = b_stds\n",
    "    df['Mean']['Mean'],df['Mean']['Std'] = None,None\n",
    "    df['Std']['Mean'],df['Std']['Std'] = None,None\n",
    "    return df\n",
    "\n",
    "sorted_dtc_df = sorted_df_with_mean_and_std(dtc_df)\n",
    "sorted_dtc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figura: scoring **ordenado** del clasificador basado en arbol de decisión con medias y desciación standard por partición y bloque *  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SB0</th>\n",
       "      <th>SB1</th>\n",
       "      <th>SB2</th>\n",
       "      <th>SB3</th>\n",
       "      <th>SB4</th>\n",
       "      <th>SB5</th>\n",
       "      <th>SB6</th>\n",
       "      <th>SB7</th>\n",
       "      <th>SB8</th>\n",
       "      <th>SB9</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P0</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.046614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.052587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.056218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P3</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.044997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P4</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.044997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P5</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.046614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P6</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.034427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P7</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.054885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P8</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.042164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.063246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>0.021082</td>\n",
       "      <td>0.034427</td>\n",
       "      <td>0.021082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028109</td>\n",
       "      <td>0.035136</td>\n",
       "      <td>0.021082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &       SB0 &       SB1 &       SB2 &       SB3 &       SB4 &       SB5 &       SB6 &  SB7 &  SB8 &  SB9 &      Mean &       Std \\\\\n",
       "\\midrule\n",
       "P0   &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.000000 &  1.000000 &  1.0 &  1.0 &  1.0 &  0.960000 &  0.046614 \\\\\n",
       "P1   &  0.866667 &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.000000 &  1.0 &  1.0 &  1.0 &  0.946667 &  0.052587 \\\\\n",
       "P2   &  0.866667 &  0.866667 &  0.933333 &  0.933333 &  1.000000 &  1.000000 &  1.000000 &  1.0 &  1.0 &  1.0 &  0.960000 &  0.056218 \\\\\n",
       "P3   &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.000000 &  1.0 &  1.0 &  1.0 &  0.953333 &  0.044997 \\\\\n",
       "P4   &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.000000 &  1.0 &  1.0 &  1.0 &  0.953333 &  0.044997 \\\\\n",
       "P5   &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.000000 &  1.000000 &  1.0 &  1.0 &  1.0 &  0.960000 &  0.046614 \\\\\n",
       "P6   &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.000000 &  1.0 &  1.0 &  1.0 &  0.960000 &  0.034427 \\\\\n",
       "P7   &  0.866667 &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  1.000000 &  1.000000 &  1.0 &  1.0 &  1.0 &  0.953333 &  0.054885 \\\\\n",
       "P8   &  0.866667 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  0.933333 &  1.0 &  1.0 &  1.0 &  0.946667 &  0.042164 \\\\\n",
       "P9   &  0.866667 &  0.866667 &  0.866667 &  0.933333 &  1.000000 &  1.000000 &  1.000000 &  1.0 &  1.0 &  1.0 &  0.953333 &  0.063246 \\\\\n",
       "Mean &  0.873333 &  0.906667 &  0.926667 &  0.933333 &  0.946667 &  0.966667 &  0.993333 &  1.0 &  1.0 &  1.0 &       NaN &       NaN \\\\\n",
       "Std  &  0.021082 &  0.034427 &  0.021082 &  0.000000 &  0.028109 &  0.035136 &  0.021082 &  0.0 &  0.0 &  0.0 &       NaN &       NaN \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "           SB0       SB1       SB2       SB3       SB4       SB5       SB6  \\\n",
       "P0    0.866667  0.933333  0.933333  0.933333  0.933333  1.000000  1.000000   \n",
       "P1    0.866667  0.866667  0.933333  0.933333  0.933333  0.933333  1.000000   \n",
       "P2    0.866667  0.866667  0.933333  0.933333  1.000000  1.000000  1.000000   \n",
       "P3    0.866667  0.933333  0.933333  0.933333  0.933333  0.933333  1.000000   \n",
       "P4    0.866667  0.933333  0.933333  0.933333  0.933333  0.933333  1.000000   \n",
       "P5    0.866667  0.933333  0.933333  0.933333  0.933333  1.000000  1.000000   \n",
       "P6    0.933333  0.933333  0.933333  0.933333  0.933333  0.933333  1.000000   \n",
       "P7    0.866667  0.866667  0.933333  0.933333  0.933333  1.000000  1.000000   \n",
       "P8    0.866667  0.933333  0.933333  0.933333  0.933333  0.933333  0.933333   \n",
       "P9    0.866667  0.866667  0.866667  0.933333  1.000000  1.000000  1.000000   \n",
       "Mean  0.873333  0.906667  0.926667  0.933333  0.946667  0.966667  0.993333   \n",
       "Std   0.021082  0.034427  0.021082  0.000000  0.028109  0.035136  0.021082   \n",
       "\n",
       "      SB7  SB8  SB9      Mean       Std  \n",
       "P0    1.0  1.0  1.0  0.960000  0.046614  \n",
       "P1    1.0  1.0  1.0  0.946667  0.052587  \n",
       "P2    1.0  1.0  1.0  0.960000  0.056218  \n",
       "P3    1.0  1.0  1.0  0.953333  0.044997  \n",
       "P4    1.0  1.0  1.0  0.953333  0.044997  \n",
       "P5    1.0  1.0  1.0  0.960000  0.046614  \n",
       "P6    1.0  1.0  1.0  0.960000  0.034427  \n",
       "P7    1.0  1.0  1.0  0.953333  0.054885  \n",
       "P8    1.0  1.0  1.0  0.946667  0.042164  \n",
       "P9    1.0  1.0  1.0  0.953333  0.063246  \n",
       "Mean  1.0  1.0  1.0       NaN       NaN  \n",
       "Std   0.0  0.0  0.0       NaN       NaN  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_gnb_df = sorted_df_with_mean_and_std(gnb_df)\n",
    "sorted_gnb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figura: scoring **ordenado** del clasificador basado en redes bayesianas con medias y desciación standard por partición y bloque*  \n",
    "\n",
    "Veamos las tasas medias y varianzas del error cometido por cada clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error medio cometido por el arbol de decisión: 0.949\n",
      "Desciación típica del error cometido por el arbol de decisión: 0.055\n",
      "\n",
      "Error medio cometido por el clasif. bayesiano: 0.955\n",
      "Desciación típica del error cometido por el clasif. bayesiano: 0.045\n"
     ]
    }
   ],
   "source": [
    "print 'Error medio cometido por el arbol de decisión: %.3f' % sorted_dtc_df.loc['Mean'].mean()\n",
    "print 'Desciación típica del error cometido por el arbol de decisión: %.3f' % sorted_dtc_df.loc['Mean'].std()\n",
    "print\n",
    "print 'Error medio cometido por el clasif. bayesiano: %.3f' % sorted_gnb_df.loc['Mean'].mean()\n",
    "print 'Desciación típica del error cometido por el clasif. bayesiano: %.3f' % sorted_gnb_df.loc['Mean'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabla de diferencias tras la ordenación\n",
    "\n",
    "Pasamos a calcular las diferencias $x_i$ sobre las que podremos realizar nuestro test de hipótesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.33333333e-02  -2.00000000e-02  -6.66666667e-03   6.66666667e-03\n",
      "  -6.66666667e-03  -2.22044605e-16   6.66666667e-03   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "diferencias = sorted_dtc_df.loc['Mean'].values[:-2] - sorted_gnb_df.loc['Mean'].values[:-2] \n",
    "print diferencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realización del t-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor del estimador para nuestra muestra de diferencias: -1.350105\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "n = len(diferencias) \n",
    "d_f = n - 1\n",
    "mu = np.mean(diferencias)\n",
    "quasi_sigma =  sqrt(np.sum([(x - mu)**2 for x in diferencias]) / (n - 1))\n",
    "Z = mu / (quasi_sigma / sqrt(d_f + 1))\n",
    "print 'Valor del estimador para nuestra muestra de diferencias: %f' % Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "**Perfecto!** Pues ya tenemos todo lo que necesitábamos.\n",
    "\n",
    "En el diseño del test obtuvimos los siguientes intervalos de confianza para distintos valores de $\\alpha$:\n",
    "\n",
    "- para $\\alpha=0.1$: [-1.833, 1.833]\n",
    "- para $\\alpha=0.05$: [-2.262, 2.262]\n",
    "- para $\\alpha=0.01$: [-3.250, 3.250]\n",
    "\n",
    "El resultado arrojado por nuestro estadístico ha sido $z=-1.350105$. Incluido incluso en el intervalo de confianza más desfavorable a la hipótesis, $\\alpha = 0.1$\n",
    "\n",
    "**Por lo tanto, dada la muestra de datos de diferencias que hemos calculado anteriormente, no tenemos evidencias suficientes para descartar la hipótesis $H_0$  que decía que los clasificadores A y B ofrecen similares resultados en el conjunto de datos iris.**\n",
    "\n",
    "O explicado de otro modo, si asumimos como cierta la hipótesis $H_0$, **los resultados de nuestro muestreo no son en absoluto extremos,** son perfectamente coherentes con tal hipótesis, y por lo tanto no hay evidencia suficiente para descartarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "- Bouckaert, R. (2004). Estimating Replicability of Classifier Learning Experiments, ICML,\n",
    "\n",
    "- Lara Porras A.M. (2002). “Estadística para Ciencias Biológicas y Ciencias Ambientales. Problemas y Exámenes Resueltos”. Ed. Proyecto Sur.\n",
    "\n",
    "- Martín Andrés, A. y Luna del Castillo, J. de D. (1990). “Bioestadística para las Ciencias de la Salud”. Ed. Norma\n",
    "\n",
    "- http://en.wikipedia.org/wiki/Student's_t-test\n",
    "\n",
    "- http://mathworld.wolfram.com/Pairedt-Test.html\n",
    "\n",
    "- https://www.uoc.edu/in3/emath/docs/CH_1Pob.pdf\n",
    "\n",
    "- http://stattrek.com/probability-distributions/t-distribution.aspx\n",
    "\n",
    "- http://businessjargons.com/t-distribution.html\n",
    "\n",
    "- http://businessjargons.com/hypothesis-testing.html\n",
    "\n",
    "- http://www.dmae.upct.es/~mcruiz/Telem06/Teoria/contrastes_06b.pdf\n",
    "\n",
    "- http://sauce.pntic.mec.es/~jpeo0002/Archivos/PDF/T05.pdf\n",
    "\n",
    "- http://www.dm.uba.ar/materias/estadistica_Q/2010/2/C011Tests%20para%20dos%20muestras.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 4.1: Ejercicios de simulación\n",
    "\n",
    "El estudiante utilizará weka para generar 10 particiones de 10 bloques cada una del conjunto de datos \"iris.arff\" proporcionado junto con el software de la Universidad de Waikato. Para cada partición, deberá realizar un experimento de validación cruzada con un clasificador basado en redes bayesianas y otro en árboles de decisión, y deberá ordenar las diferencias de rendimiento de mayor a menor en una lista, según el ejemplo Average over sorted runs del artículo Bouckaert, R. (2004). Estimating Replicability of Classifier Learning Experiments, ICML, Disponible en: http://www.aicml.cs.ualberta.ca/_ban_04/icml/pages/papers/61.pdf . \n",
    "\n",
    "Deberá realizar un test t de Student (¡ojo! el texto base tiene un error en esa sección; recomendamos que se busque cualquier texto base de estadística o la misma wikipedia para identificar a qué contexto particular pertenece el problema (una cola, dos colas, muestras dependientes, independientes...) y la metodología a seguir) que determine si existen diferencias estadísticas entre los resultados obtenidos por los dos clasificadores.\n",
    "\n",
    "Entregables: El estudiante deberá entregar un trabajo de entre 10 y 15 páginas A4 a una cara con los siguientes apartados:\n",
    "\n",
    "- Descripción del experimento.\n",
    "- Tablas ordenadas de cada uno de los 10 experimentos de validación cruzada para cada clasificador.\n",
    "- Valores promediados de la tabla anterior.\n",
    "- Cálculo de la tasa media de error y su varianza para cada clasificador. \n",
    "- Definición (con fórmulas) del test t de Student. Cálculo y resultado del test de Student.\n",
    "\n",
    "La distribución t de Student se puede obtener de muchas fuentes. En particular, el estudiante puede hallarla implementada en la librería gsl de GNU para c/c++.\n",
    "\n",
    "\n",
    "Bibliografía asociada: \n",
    "\n",
    "- Bouckaert, R. (2004). Estimating Replicability of Classifier Learning Experiments, ICML,\n",
    "- Para el test pareado de Student se puede consultar el texto 'Estadística. Modelos y Métodos' de Daniel Peña Sánchez de Rivera o las entradas correspondientes de la enciclopedia matemática on-line Mathworld http://mathworld.wolfram.com/Pairedt-Test.html o de la wikipedia, http://en.wikipedia.org/wiki/Student's_t-test.\n",
    "\n",
    "Rúbricas:\n",
    "\n",
    "- Fundamentos teóricos\t\t\t\n",
    "\t- Comparación de clasificadores\t\t\n",
    "\t\t- Breve resumen teórico\t0,625\n",
    "\t- Aspectos clave de la técnica\t\t\n",
    "\t\t- Test t de student – Resumen teórico\t0,625\n",
    "\t\t- Normalidad muestras\t0,625\n",
    "\t\t- Pareado/No pareado\t0,625\n",
    "\t\t- Una/dos colas\t0,625\n",
    "\t\t- Fórmulas\t0,625\n",
    "- Ejecución de la práctica\t\t\t\n",
    "\t- Experimento de CV\t\t\n",
    "\t\t- Generación 100 particiones\t1,250\n",
    "\t\t- Ordenación por bloques\t0,625\n",
    "\t\t- Cálculo de restas\t0,625\n",
    "\t- Test t de student\t\t\n",
    "\t\t- Elección correcta de fórmula\t1,250\n",
    "\t\t- Cálculo correcto de d.f.\t0,625\n",
    "\t\t- Cálculo correcto de sigma\t0,625\n",
    "- Intepretación de los resultados\t\t\t\n",
    "\t- Interpretación correcta\t1,250\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "navigate_menu": false,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
